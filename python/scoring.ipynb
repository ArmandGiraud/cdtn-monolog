{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import json\n",
    "\n",
    "pd.options.display.max_rows = 400\n",
    "pd.options.display.max_colwidth = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_searches = pd.read_excel(\"searches_candidates_selection_rank.xlsx\")\n",
    "df_searches = df_searches[df_searches['query'] != 'undefined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slug_only(dr):\n",
    "    candidates = json.loads(dr.replace(\"'\", '\"'))    \n",
    "    try :\n",
    "        return json.dumps([c['slug'] for c in candidates])\n",
    "    except TypeError:\n",
    "        print('ERROR:\\n' + dr)\n",
    "        return None\n",
    "\n",
    "df_searches['docs_slug'] = df_searches.docs_res.apply(lambda dr : slug_only(dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_searches_unique = df_searches[[\"rank\", \"docs_slug\", \"query\", \"docs_res\", \"idVisit\"]]\n",
    "df_searches_unique.drop_duplicates([\"docs_slug\", \"query\", \"idVisit\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_searches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation : group queries and get rank for each candidates \n",
    "Objective : compute NDCG / MAP / MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# most frequent queries with hit\n",
    "all_ranks = []\n",
    "\n",
    "threshold = 5\n",
    "for g, rows in df_searches_unique[df_searches['rank'] >= 0].groupby([\"docs_slug\"]):\n",
    "    try:\n",
    "        n = rows.shape[0]\n",
    "        q = set(rows['query'].tolist())    \n",
    "        algos = [docres['algo'] for docres in json.loads(rows.docs_res.tolist()[0].replace(\"'\",'\"'))]\n",
    "        #algos = \n",
    "        preq = 'pre-qualified' in algos\n",
    "        candidates = g\n",
    "        if (n > threshold):        \n",
    "            print(f'{q} -> {n}')\n",
    "            #print(algo)\n",
    "            ranks = rows['rank'].value_counts().sort_index()\n",
    "            candidates_array = json.loads(candidates)\n",
    "            j = len(candidates_array)\n",
    "            complete_ranks = [0] * j\n",
    "            ids = []\n",
    "            for i in range(0, j):\n",
    "                if i in ranks.index:\n",
    "                    complete_ranks[i] = ranks[i]             \n",
    "                ids.append({'slug':candidates_array[i], 'count':int(complete_ranks[i]), 'preq':algos[i]})\n",
    "            all_ranks.append({'total':int(n), 'queries':list(q), 'preq': preq, 'docs':ids})\n",
    "    except KeyError:\n",
    "        print(\"Key error : \" + g)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(all_ranks))\n",
    "#print()\\\n",
    "#print(sorted(all_ranks[6], key=lambda x: x['count'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(all_ranks[15], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from documentation\n",
    "\n",
    "params = {\n",
    "    \"y_pred\" : [\"a\", \"b\", \"c\", \"w\", \"k\",\"e\"], # y_pred (array): list of documents id predicted by the system\n",
    "    \"y_true\" : [\"a\", \"b\", \"c\", \"e\"], # y_true (array): documents id scored by humans sorted from most relevant to least relevant\n",
    "    \"y_score\" : {\n",
    "        \"a\":5,\n",
    "        \"b\":3,\n",
    "        \"c\":2,\n",
    "        \"e\":2\n",
    "            }, # y_score is a dict {\"doc_id\":\"score\"} of documents assigned as relevant y humans with the associated scores (ordered internally)\n",
    "    \"method\" : \"all\", # one of [\"precision\", \"recall\", \"dcg\", \"mrr\", \"all\"]\n",
    "    \"k\": 3 # k integer preferaly =< to length(y_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_ranks(ranks):\n",
    "    return sorted(ranks, key=lambda x: x['count'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used anymore\n",
    "'''\n",
    "def compute_metrics(rank_results, i, verbose=False, api=False):\n",
    "    scores = dict([(r['slug'], int(r['count'])) for r in rank_results[i]['docs']])\n",
    "    if not api:\n",
    "        y_pred = [c['slug'] for c in rank_results[i]['docs']]\n",
    "    elif api:\n",
    "        y_pred = rank_results[i]['docs_api']\n",
    "        \n",
    "    params = {\n",
    "\n",
    "        \"y_pred\": y_pred,\n",
    "        #\"y_pred\": [c['slug'] for c in sort_ranks(all_ranks[i]) if c['count'] > 0],\n",
    "        \"y_true\" : [c['slug'] for c in sort_ranks(rank_results[i]['docs']) if c['count'] > 0],\n",
    "        \"y_score\" : scores,\n",
    "        \"method\" : \"all\",\n",
    "        \"k\":5\n",
    "    }\n",
    "    r = requests.post(\"http://localhost:4545/api/score\", json = params)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        print(\"Error with at index \" + str(i))        \n",
    "        result = {'dcg': 1.0, 'mrr': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
    "    else :\n",
    "        result = r.json()\n",
    "        \n",
    "    if verbose:\n",
    "        print(i)\n",
    "        print(rank_results[i][0]['queries'])\n",
    "        print(json.dumps(scores, indent=2))\n",
    "        print(result)\n",
    "    \n",
    "    result['occ'] = rank_results[i]['total']\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from Armand's letor\n",
    "def discounted_cumulative_gain(y_score, y_true, y_pred, k):\n",
    "    \"\"\"y_score is a dictionnary {\"doc_id\":\"score\"} of documents assigned as relevant y humans with the associated scores\n",
    "       k = maximum index to be scored\n",
    "       /!\\ to be valid, dcg should have results lists of same length bbetween requests\"\"\"\n",
    "    if y_pred == []:\n",
    "        return 0\n",
    "    if k > len(y_pred):\n",
    "        k = len(y_pred)\n",
    "    \n",
    "    y_scoring = []\n",
    "    for y in y_pred:\n",
    "        score = y_score.get(y)\n",
    "        if score is None: # if the predicted document is not in the array of humanly scored documents\n",
    "            score = 0\n",
    "        y_scoring.append(score)\n",
    "    \n",
    "    # compute dcg\n",
    "    def compute_dcg(y_scoring, k):\n",
    "        dcg = []\n",
    "        for i, pred in enumerate(y_scoring[:k]):\n",
    "            i+=1\n",
    "            # two formulas possible, according to wikipedia the latter places stronger emphasis on retrieving relevant documents\n",
    "            dcg.append(pred/(np.log2(i)+1))\n",
    "            #dcg.append((np.power(pred,2) - 1)/(np.log2(i)+1))\n",
    "        return np.sum(dcg)\n",
    "    # compute Ideal DCG\n",
    "    # ideal DCG: the best score that could have been obtained given the relevant document list\n",
    "    # i. e. : the most relevant documents, ordered by relevance.\n",
    "\n",
    "    dcg = compute_dcg(y_scoring, k)\n",
    "\n",
    "    ideal_scores = []\n",
    "    for i in y_true:\n",
    "        score = y_score.get(i)\n",
    "        if not i in y_score : # if we don't have a score for y true\n",
    "            y_score[i]\n",
    "            raise ValueError(\"the true document does not have score in y_score : \" + i)\n",
    "        ideal_scores.append(score)\n",
    "\n",
    "    ideal_scores = sorted(ideal_scores, reverse=True)\n",
    "\n",
    "    ideal_dcg = compute_dcg(ideal_scores, k)\n",
    "    return dcg/ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update candidates with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each group, trigger query \n",
    "all_ranks[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.iloc[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[ar for ar in range(len(all_ranks)) if 'pr√©avis cdd' in all_ranks[ar]['queries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_slug(source, slug) :\n",
    "    formated_source = source.replace('_', '-', )\n",
    "    #print(formated_source)\n",
    "    #print(source)\n",
    "    formated_source = formated_source.replace('fiches', 'fiche')\n",
    "    if (source == 'contributions'):\n",
    "        formated_source = formated_source[:-1]\n",
    "        \n",
    "    return f\"/{formated_source}/{slug}\"\n",
    "\n",
    "def call_api(query):\n",
    "    r = requests.get(\"http://localhost:1337/api/v1/search?q=\" + query)\n",
    "    if r.status_code != 200:\n",
    "        print(\"Error when calling search API for query : \" + str(q))        \n",
    "        return {}\n",
    "    else :\n",
    "        result = r.json()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_call(i):\n",
    "    try:\n",
    "        api_res = call_api(all_ranks[i]['queries'][0])['documents']       \n",
    "        call_res = [reformat_slug(d['source'], d['slug']) for d in api_res]\n",
    "        origin_res = [d['slug'] for d in all_ranks[i]['docs']]\n",
    "        #if (call_res != origin_res):\n",
    "        #    return False\n",
    "            #print([call_res[j] == origin_res[j] for j in range(len(call_res))])\n",
    "            #print(json.dumps(call_res, indent=2))\n",
    "            #print(json.dumps(origin_res, indent=2))\n",
    "\n",
    "        #else:\n",
    "        all_ranks[i]['docs_api'] = call_res\n",
    "        return True\n",
    "    except KeyError:\n",
    "        return False\n",
    "    except IndexError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "issues = []\n",
    "for i in range(0, len(all_ranks)):\n",
    "#for i in range(0, 2):\n",
    "    if not control_call(i):\n",
    "        issues.append(i)\n",
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_preq_issues = []\n",
    "for i in issues:\n",
    "    if not all_ranks[i]['preq']:\n",
    "        #print(i)\n",
    "        non_preq_issues.append(i)\n",
    "len(non_preq_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for now we'll only consider calls without issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_issues_ranks = [j for i, j in enumerate(all_ranks) if i not in issues]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_issues_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_issues_ranks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(0, len(no_issues_ranks)) if not 'docs_api' in no_issues_ranks[i].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dcg_local(rank_results, i, verbose=False, api=False):\n",
    "    scores = dict([(r['slug'], int(r['count'])) for r in rank_results[i]['docs']])\n",
    "    is_preq = rank_results[i]['preq']\n",
    "    occ = sum([c for (_,c) in scores.items()])\n",
    "    if not api:\n",
    "        y_pred = [c['slug'] for c in rank_results[i]['docs']]\n",
    "    elif api:\n",
    "        y_pred = rank_results[i]['docs_api']\n",
    "    y_true = [c['slug'] for c in sort_ranks(rank_results[i]['docs'])]\n",
    "    ndcg = discounted_cumulative_gain(scores, y_true, y_pred, 7)\n",
    "    if verbose :\n",
    "        print(json.dumps(scores, indent=2))\n",
    "        print(json.dumps(y_pred, indent=2))\n",
    "        print(json.dumps(y_true, indent=2))\n",
    "        print(ndcg)\n",
    "    return {'dcg': ndcg, 'occ': occ, 'preq': is_preq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retest all\n",
    "\n",
    "j = 0\n",
    "issues = []\n",
    "#for i in range(0, len(all_ranks)):\n",
    "#    if not control_call(i):\n",
    "#        issues.append(i)\n",
    "len(issues)\n",
    "all_scores = pd.DataFrame([compute_dcg_local(no_issues_ranks, i, api=False) for i in range(len(no_issues_ranks))])\n",
    "all_scores['weighted_dcg'] = all_scores['dcg'] * all_scores['occ'] \n",
    "\n",
    "preq_scores = all_scores[all_scores.preq]\n",
    "other_scores = all_scores[~all_scores.preq]\n",
    "\n",
    "print(f\"nDCG all : {compute_weighted_dcg(all_scores)}\")\n",
    "print(f\"nDCG preq : {compute_weighted_dcg(preq_scores)}\")\n",
    "print(f\"nDCG other : {compute_weighted_dcg(other_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : we need to ponderate DCGs on count (as DCG of a frequent query should be higher than other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_api = pd.DataFrame([compute_dcg_local(no_issues_ranks, i, api=True) for i in range(len(no_issues_ranks))])\n",
    "all_scores_api['weighted_dcg'] = all_scores_api['dcg'] * all_scores_api['occ'] \n",
    "all_scores[\"weighted_api\"] = all_scores_api[\"weighted_dcg\"]\n",
    "all_scores[\"dcg_api\"] = all_scores_api[\"dcg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores[~all_scores.preq].sort_values(by=['occ', 'dcg'], ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[~all_scores.preq]['occ'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[all_scores.preq]['occ'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 269\n",
    "no_issues_ranks[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.loc[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dcg_local(no_issues_ranks, id, verbose=True, api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_weighted_dcg(scores):\n",
    "    return round(scores['weighted_dcg'].sum() / scores['occ'].sum(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preq_scores = all_scores[all_scores.preq]\n",
    "other_scores = all_scores[~all_scores.preq]\n",
    "\n",
    "print(f\"nDCG all : {compute_weighted_dcg(all_scores)}\")\n",
    "print(f\"nDCG preq : {compute_weighted_dcg(preq_scores)}\")\n",
    "print(f\"nDCG other : {compute_weighted_dcg(other_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preq_scores = all_scores[all_scores.preq]\n",
    "other_scores = all_scores[~all_scores.preq]\n",
    "\n",
    "print(f\"nDCG all : {compute_weighted_dcg(all_scores)}\")\n",
    "print(f\"nDCG preq : {compute_weighted_dcg(preq_scores)}\")\n",
    "print(f\"nDCG other : {compute_weighted_dcg(other_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of ranks with pre-qualified\n",
    "# index of ranks without\n",
    "\n",
    "preq_indices = [i for i in range(0, len(no_issues_ranks)) if no_issues_ranks[i]['preq']]\n",
    "other_indices = [i for i in range(0, len(no_issues_ranks)) if not no_issues_ranks[i]['preq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preq_indices))\n",
    "print(len(other_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preq_scores = [compute_dcg_local(no_issues_ranks, i) for i in preq_indices]\n",
    "#other_scores = [compute_dcg_local(no_issues_ranks, i, api=False) for i in other_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_scores = [compute_metrics(no_issues_ranks, i, api=True) for i in range(len(no_issues_ranks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weighted average\n",
    "# show : all / preq only / other / fulltex / sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show : only fulltext / only sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(all_scores)['dcg'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(all_scores)['dcg'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(preq_scores)['dcg'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(other_scores)['dcg'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(preq_scores + other_scores)['dcg'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_preq = False\n",
    "if run_preq:\n",
    "    scores = preq_scores\n",
    "    indices = preq_indices\n",
    "else:\n",
    "    scores = other_scores\n",
    "    indices = other_indices\n",
    "\n",
    "min_score = 0.98\n",
    "min_occ = 5\n",
    "\n",
    "to_check_indices = [(i, round(scores.iloc[i]['dcg'], 2), scores.iloc[i]['occ']) for i in range(0, len(scores)) if scores.iloc[i]['dcg'] < min_score and scores.iloc[i]['occ'] > min_occ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "test_i = to_check_indices[10][0]\n",
    "pi = indices[test_i]\n",
    "print(scores.iloc[test_i])\n",
    "print(json.dumps(no_issues_ranks[pi], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_entry(i, queries, candidates_rank, score, count):\n",
    "    lines = []\n",
    "    lines.append(f\"## ({i}) {list(queries)[0]}\")\n",
    "    lines.append(f'Score : `{score}`')\n",
    "    lines.append(f'Count : `{count}`\\n')\n",
    "    lines.append('Variantes :')\n",
    "    lines.extend(['- ' + q for q in queries])\n",
    "    lines.append('')\n",
    "    lines.append('|count|slug|algo|')\n",
    "    lines.append('|--|--|--|')\n",
    "    lines.extend([f\"{ar['count']} | {ar['slug']} | {ar['preq']}\" for ar in candidates_rank['docs']])\n",
    "    lines.append('\\n')\n",
    "    #lines.extend([f\"{ar}\" for ar in candidates_rank['docs_api']])\n",
    "    lines.append('\\n\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "# sort by count\n",
    "to_check_indices.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for j in range(len(to_check_indices)) :\n",
    "    (i, score, count) = to_check_indices[j]\n",
    "    candidates_rank = no_issues_ranks[indices[i]]\n",
    "    queries = candidates_rank['queries']\n",
    "    \n",
    "    all_lines.extend(format_entry(j, queries, candidates_rank, score, count))    \n",
    "\n",
    "#print(\"\\n\".join(all_lines))\n",
    "#print(all_lines)\n",
    "#[print(f\"{ar['count']} => {ar['slug']}\") for ar in candidates_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df-updates2.md\", 'w+') as f :\n",
    "    f.write(\"\\n\".join(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"search-issues-api2.md\", 'w+') as f :\n",
    "    f.write(\"\\n\".join(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranks[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check best semantic entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sem_best(i):\n",
    "    docs = no_issues_ranks[i]['docs']\n",
    "    docs.sort(key=lambda x: x['count'], reverse=True)\n",
    "    if (len(docs) > 1):\n",
    "        return (docs[0]['preq'] == 'semantic') # or (docs[1]['preq'] == 'semantic')\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "sem_matches = [i for i in range(0, len(no_issues_ranks)) if sem_best(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sem_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(no_issues_ranks[i]['queries'][0], no_issues_ranks[i]['docs'][0]['count'], call_api(no_issues_ranks[i]['queries'][0])['documents'][0]['_score']) for i in sem_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_issues_ranks[211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
